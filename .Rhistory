##################################################################################
# Project:      Credit Scoring Analysis (petit example)
# Description:  Part 5 - Cluster Analysis
# Data:         CleanCreditScoring.csv
# By:           Gaston Sanchez
# url:          www.gastonsanchez.com
# Perform a cluster analysis on the results obtained
# Note:
#     Check Part 4 - Multiple Correspondence Analysis
#     Part4_CredScoring_MCA.R
# remember to change your working directory!!! (don't use mine)
# setwd("/Users/gaston/Documents/Gaston/StatsDataMining")
# load package FactoMineR and ggplot2
require(ggplot2)
# read cleaned data set
dd = read.csv("CleanCreditScoring.csv", header=TRUE, stringsAsFactors=TRUE)
# Apply MCA on categorical variables
# ================================================================================
assetsR, debtR, amountR, priceR, finratR, savingsR, Status))
# MCA
mca = MCA(ddcat, ncp=40, quali.sup=12, graph=FALSE)
ddcat = subset(dd, select=c(seniorityR, timeR, ageR, expensesR, incomeR,
# from the multiple correspondence analysis
# get the eigenvalues
eigs = mca$eig$eigenvalue
# calculate significant dimension in MCA
require(FactoMineR)
nd = sum(eigs>1/length(eigs))
# get factorial coordinates of individuals
# select select categorized continuous variables
##################################################################################
# ================================================================================
Psi = mca$ind$coord
# ================================================================================
#
# K-means clustering on factorical coordinates from MCA results
#
# ================================================================================
# The idea is to take the extracted dimensions (i.e. the factorial cooridnates)
# from the MCA in order to perform a k-means cluster analysis on them
# The first approach is to apply K-means on factorial coordinates
# Let's try k=5 groups
k1 = kmeans(Psi, 5)
# what does k1 contain?
attributes(k1)
# examine the following
k1$size       # size of clusters
k1$withinss   # within cluster variance
k1$centers    # centers coordinates
# between clusters sum of squares
Bss = sum(rowSums(k1$centers^2) * k1$size)
Bss
# within clusters sum of squares
Wss = sum(k1$withinss)
Wss
# total sum of squares
Tss = sum(rowSums(Psi^2))
Tss
Bss + Wss
# let's calculate the decomposition of inertia
setwd("C:/Users/Conlana/Desktop/Scorecards/ScoringR")
##################################################################################
# Project:      Credit Scoring Analysis (petit example)
# Description:  Part 5 - Cluster Analysis
# By:           Gaston Sanchez
#
# from the multiple correspondence analysis
#
# Data:         CleanCreditScoring.csv
#     Check Part 4 - Multiple Correspondence Analysis
#     Part4_CredScoring_MCA.R
# Note:
# remember to change your working directory!!! (don't use mine)
# setwd("/Users/gaston/Documents/Gaston/StatsDataMining")
# load package FactoMineR and ggplot2
require(FactoMineR)
require(ggplot2)
##################################################################################
# read cleaned data set
# url:          www.gastonsanchez.com
# ================================================================================
dd = read.csv("CleanCreditScoring.csv", header=TRUE, stringsAsFactors=TRUE)
# Apply MCA on categorical variables
# Perform a cluster analysis on the results obtained
# ================================================================================
# select select categorized continuous variables
ddcat = subset(dd, select=c(seniorityR, timeR, ageR, expensesR, incomeR,
assetsR, debtR, amountR, priceR, finratR, savingsR, Status))
# MCA
mca = MCA(ddcat, ncp=40, quali.sup=12, graph=FALSE)
# get the eigenvalues
eigs = mca$eig$eigenvalue
# calculate significant dimension in MCA
nd = sum(eigs>1/length(eigs))
# get factorial coordinates of individuals
Psi = mca$ind$coord
# ================================================================================
# K-means clustering on factorical coordinates from MCA results
# ================================================================================
# The idea is to take the extracted dimensions (i.e. the factorial cooridnates)
# from the MCA in order to perform a k-means cluster analysis on them
# The first approach is to apply K-means on factorial coordinates
# Let's try k=5 groups
k1 = kmeans(Psi, 5)
# what does k1 contain?
attributes(k1)
# examine the following
k1$size       # size of clusters
k1$withinss   # within cluster variance
k1$centers    # centers coordinates
# between clusters sum of squares
Bss = sum(rowSums(k1$centers^2) * k1$size)
Bss
# within clusters sum of squares
Wss = sum(k1$withinss)
Wss
# total sum of squares
Tss = sum(rowSums(Psi^2))
Tss
Bss + Wss
# let's calculate the decomposition of inertia
Ib1 = 100 * Bss / (Bss + Wss)
Ib1
# let's repeat kmeans, again with k=5
k2 = kmeans(Psi, 5)
# between clusters sum of squares
Bss = sum(rowSums(k2$centers^2) * k2$size)
Wss = sum(k2$withinss)
# total sum of squares
Tss = sum(rowSums(Psi^2))
Bss + Wss    # Tss = Bss + Wss
# let's calculate the decomposition of inertia
Ib2 = 100 * Bss / (Bss + Wss)
Ib2
# why are we obtaining different results?  (Ib1 != Ib2)
# you can keep playing with different values for k
# let's repeat kmeans, again with k=8
k3 = kmeans(Psi, 8)
# between clusters sum of squares
Bss = sum(rowSums(k3$centers^2) * k3$size)
Wss = sum(k3$withinss)
# total sum of squares
Tss = sum(rowSums(Psi^2))
Bss + Wss    # Tss = Bss + Wss
# let's calculate the decomposition of inertia
Ib3 = 100 * Bss / (Bss + Wss)
Ib3
# ================================================================================
# Hierarchical clustering on factorical coordinates from MCA results
# ================================================================================
# Now, let's apply a hierarchical clustering
# first we calculate a distance matrix between individuals
idist = dist(Psi)
# then we apply hclust with method="ward"
# notice the computation cost! (it takes a while to finish)
h1 = hclust(idist, method="ward")
# plot dendrogram
plot(h1, labels=FALSE)
# after checking the dendrogram, how many groups would you choose?
# where would you cut the dendrogram?
# let's try 8 clusters
nc = 8
# let's cut the tree and see the size of clusters
c1 = cutree(h1, nc)
table(c1)
# prepare data frame for ggplot
df1 = data.frame(Status=dd$Status, mca$ind$coord[,1:2], cluster=as.factor(c1))
# visualize clusters using the first two factorial coordinates
ggplot(data=df1, aes(x=Dim.1, y=Dim.2)) +
geom_hline(yintercept=0, colour="gray65") +
geom_vline(xintercept=0, colour="gray65") +
geom_point(aes(colour=cluster), alpha=0.5) +
labs(x="Dim 1", y="Dim 2") +
ggtitle("MCA plot with clusters of individuals")
# centers of gravity of the clusters
cog = aggregate(as.data.frame(Psi), list(c1), mean)[,-1]
# what's the quality of the hierarchical partition?
Bss = sum(rowSums(cog^2) * table(c1))
Ib4 = 100 * Bss / Tss
Ib4
# ================================================================================
# Combining k-means and hierarchical clustering with MCA results
# ================================================================================
# let's consolidate the partition
# we'll apply  k-means using the cog's from the hierarchical clustering
k5 = kmeans(Psi, center=cog)
k5$size
Bss = sum(rowSums(k5$centers^2) * k5$size)
Wss = sum(k5$withinss)
Ib5 = 100 * Bss / (Bss + Wss)
Ib5
# clustering of large data sets
# first 2 kmeans with k=14
n1 = 14
km1 = kmeans(Psi, n1)
km2 = kmeans(Psi, n1)
# what's the overlapping between clusters?
table(km2$cluster, km1$cluster)
clas = (k2$cluster - 1)*n1 + k1$cluster
freq = table(clas)
freq[1:10]
# what do we have in freq?
cogclas <- aggregate(as.data.frame(Psi), list(clas), mean)[,2:(ncol(Psi)+1)]
# perform a hierarchical clustering using cogclas
# compare the computational cost (this is way much faster!)
d2 = dist(cogclas)
h2 = hclust(d2, method="ward", members=freq)
# dendrogram
plot(h2)
# barplot
barplot(h2$height)
# cut tree in nc=8 groups
c2 <- cutree(h2, nc)
# ================================================================================
# Probabilistic clustering on MCA results
# ================================================================================
# load package mclust
library(mclust)
# check the computational cost
emc <- Mclust(Psi, G=7:9)
print(emc)
attributes(emc)
# In this case, we have a probability for each individual
emc$z[1:10,]
# let's see the membership for every individual
emc$classification[1:10]
table(emc$classification)
# what's the quality of the partition?
cog <- aggregate(as.data.frame(Psi), list(emc$classification), mean)[,2:(ncol(Psi)+1)]
Bss <- sum(rowSums(cog^2)*as.numeric(table(c1)))
Ib7 <- 100*Bss/Tss
Ib7
# add emc classification to data frame
df1$EMC.class = as.factor(emc$classification)
# visualize clusters using the first two factorial coordinates
ggplot(data=df1, aes(x=Dim.1, y=Dim.2)) +
geom_hline(yintercept=0, colour="gray65") +
geom_vline(xintercept=0, colour="gray65") +
geom_point(aes(colour=EMC.class), alpha=0.5) +
labs(x="Dim 1", y="Dim 2") +
ggtitle("MCA plot with clusters of individuals")
######################################################
rm(list=ls())
# Part 5: DECISION TREES
######################################################
# Part 5: DECISION TREES
# Apply Decision Trees Analysis
#
#
######################################################
# load package FactoMineR and ggplot2
require(ggplot2)
require(rpart)
# read cleaned data set
dd = read.csv("CleanCreditScoring.csv", header=TRUE, stringsAsFactors=TRUE)
# Let's obtain a decision tree with the function 'rpart'
# using all the variables (both continuous and categorical)
ct = rpart(Status ~ ., data=dd)
# let's see how the output looks like
ct
# how to read the output?
# "node), split, n, loss, yval, (yprob)"
# node): indicates the node number
# split: indicates the split criterion
# n: indicates the number of individuals in the groupe
# loss: indicates the the number of individuals misclassified
# yval: indicates the predicted value
# (yprob): indicates the probability of belonging to each class
# it's much easier to read a tree with a graphic
plot(ct, margin=0.05, compress=TRUE, main="Decision Tree")
text(ct, use.n=TRUE, pretty=1, all=TRUE, cex=0.7)
# one of the goals is to obtain a tree in which
# the nodes are as much homogenous as possible,
# but also a tree with good prediction ability
# In order to improve our decision tree, we need to have
# 1) a train (aka learning) dataset
# 2) a test dataset
# let's keep 2/3 of the data for learning, and 1/3 for testing
n = nrow(dd)
learn = sample(1:n, size=round(0.67 * n))
nlearn = length(learn)
ntest = n - nlearn
# selection of model by crossvalidation
# first we need a maximal tree with low value of cp
# and quiprobability of classes
ct1 = rpart(Status ~ ., data = dd[learn,], method="class",
parms = list(prior=c(0.50, 0.50), split='gini'),
control = rpart.control(cp=0.001, xval=10, maxdepth=15))
# check results of the complexity parameter table
ct1$cptable
# we can use the function 'plotcp' to see the results
# the 'best' tree is the one with the lowest xerror
plotcp(ct1, las=2, cex.axis=0.8)
# what is the minimum XERROR?
min(ct1$cptable[,4])
min.xe = which(ct1$cptable[,4] == min(ct1$cptable[,4]))
# the optimal tree corresponds to a cp=0.003
ct1$cptable[min.xe,]
# Now that we know that the 'best' tree has cp=0.03
# we can plugin that information in the parameters
ct2 = rpart(Status ~ .,
data = dd[learn,],
parms = list(prior=c(0.50, 0.50), split='gini'),
control = rpart.control(cp=0.00285, xval=0, maxdepth=15))
# plot
par(mar = c(1,1,2,0.5))
plot(ct2, margin=0.05, compress=TRUE, main="Decision Tree")
text(ct2, use.n=TRUE, pretty=1, all=TRUE, cex=0.5)
summary(ct2)
# calculate error rate in the learning sample
# (this will give a matrix)
ct2.learn = predict(ct2, data=ddtot[learn,])
# create a vector with predicted status
ct2.learnp = rep("", nlearn)
ct2.learnp[ct2.learn[,1] < 0.5] = "pred_neg"
ct2.learnp[ct2.learn[,1] >= 0.5] = "pred_pos"
# let's make a table
status_learn = table(dd$Status[learn], ct2.learnp)
# classification error
100 * sum(diag(status_learn)) / nlearn
# calculate error rate in the testing sample
# (this will give a matrix)
ct2.test = predict(ct2, newdata=ddtot[-learn,])
# create a vector with predicted status
ct2.testp = rep("", ntest)
ct2.testp[ct2.test[,1] < 0.5] = "pred_neg"
ct2.testp[ct2.test[,1] >= 0.5] = "pred_pos"
# let's make a table
status_test = table(dd$Status[-learn], ct2.testp)
# classification error
100 * sum(diag(status_test)) / ntest
# we'll repeat the same but changing the cp=0.002
ct3 = rpart(Status ~ .,
data = dd[learn,],
parms = list(prior=c(0.50, 0.50), split='gini'),
control = rpart.control(cp=0.002, xval=0, maxdepth=15))
par(mar = c(1,1,2,0.5))
plot(ct3, margin=0.05, compress=TRUE, main="Decision Tree")
text(ct3, use.n=TRUE, all=TRUE, cex=0.5)
# calculate error rate in the learning sample
# (this will give a matrix)
ct3.learn = predict(ct3, data=ddtot[learn,])
# create a vector with predicted status
ct3.learnp = rep("", nlearn)
ct3.learnp[ct3.learn[,1] < 0.5] = "pred_neg"
ct3.learnp[ct3.learn[,1] >= 0.5] = "pred_pos"
# let's make a table
table(dd$Status[learn], ct3.learnp)
# classification error
100 * sum(diag(table(dd$Status[learn], ct3.learnp))) / nlearn
# calculate error rate in the testing sample
# (this will give a matrix)
ct3.test = predict(ct3, newdata=ddtot[-learn,])
# create a vector with predicted status
ct3.testp = rep("", ntest)
ct3.testp[ct3.test[,1] < 0.5] = "pred_neg"
ct3.testp[ct3.test[,1] >= 0.5] = "pred_pos"
# let's make a table
table(dd$Status[-learn], ct3.testp)
# classification error
100 * sum(diag(table(dd$Status[-learn], ct3.testp))) / ntest
# concentration curve
# the positive predictions on the test sample
pred.test = ct2.test[,1]
# the number of individuals in each value
totn = table(-pred.test) / ntest
ac_totn = 100 * cumsim(as.numeric(totn))
# ranking the predictions
rank_pred.test = rank(pred.test)
# how many positive are in each leave?
Status.test = dd$Status[-learn]
table(Status.test)
npos = table(Status.test)[1]
tapply(Status.test == "good", ran_pred.test, sum)
ac_true.pos = 100 * cumsum(rev(as.numeric(tabla))) / npos
# ROC curve
nneg = ntest - npos
ac_fals.pos = 100 * cumsum(rev())
rm(list=ls())
require(FactoMineR)
require(ggplot2)
# read cleaned data set
dd = read.csv("CleanCreditScoring.csv", header=TRUE, stringsAsFactors=TRUE)
# ================================================================================
# Apply logistic regression using all the variables
# ================================================================================
# let's keep 2/3 of the data for learning, and 1/3 for testing
n = nrow(dd)
learn = sample(1:n, size=round(0.67 * n))
nlearn = length(learn)
ntest = n - nlearn
# "whole enchilada" logistic regression model
gl1 = glm(Status ~ ., data=dd[learn,], family=binomial)
# use the 'summary' function to obtain more details on the logistic model
# (pay attention to the signicance of the coefficients)
summary(gl1)
# let's use the 'anova' function to get a sequential analysis of
# variance of the model fit (ie importance of variables in the model)
anova(gl1)
# we can also use the 'step' function to perform model selection by
# applying a backward elimination method based on AIC (Akaike Info. Criterion)
step(gl1)
# ================================================================================
# Apply logistic regression after removing some variables
# ================================================================================
# new model
glf <- glm(formula = Status ~ Seniority + Age + Income + Debt + Amount + Finrat +
seniorityR + expensesR + assetsR + priceR + savingsR + Home + Marital + Records +
Job, family = binomial, data = dd[learn, ])
# check summary
summary(glf)
# check coefficients
exp(glf$coefficients)
# re-expressed fitted values
glf$fitted.values = 1 - glf$fitted.values
# create vector for predictions
glfpred = rep(NA, length(glf$fitted.values))
glfpred[glf$fitted.values < 0.5] = 0
glfpred[glf$fitted.values >= 0.5] = 1
# how is the prediction? (confusion matrix)
table(dd$Status[learn], glfpred)
# error rate
error_rate.learn = 100*sum(diag(table(dd$Status[learn], glfpred))) / nlearn
error_rate.learn
# let's use the test data to get predictions
